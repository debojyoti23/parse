{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_grammar():\n",
    "    grammar_f = \"/home/debojyoti/nlp/assignment3/parse/grammar1.txt\"\n",
    "    lexicon_f = \"/home/debojyoti/nlp/assignment3/parse/lexicon.txt\"\n",
    "    pattern = re.compile(\"^(\\w+)\\s*->(.+)\")\n",
    "    terminal_rules = []\n",
    "    non_terminal_rules = []\n",
    "    nts = set([])\n",
    "    with open(lexicon_f) as lexicon:\n",
    "        for line in lexicon.readlines():\n",
    "            g_obj = pattern.match(line)\n",
    "            lhs = g_obj.group(1)\n",
    "            rhs = g_obj.group(2).strip()\n",
    "            rhs = re.split('\\W+',rhs)\n",
    "            terminal_rules.append(tuple([lhs]+rhs))\n",
    "    terminal_rules = numpy.array(terminal_rules)\n",
    "    with open(grammar_f) as grammar:\n",
    "        for line in grammar.readlines():      \n",
    "            g_obj = pattern.match(line)\n",
    "            lhs = g_obj.group(1)\n",
    "            rhs = g_obj.group(2).strip()\n",
    "            rhs = re.split('\\W+',rhs)\n",
    "            nts.update([lhs]+rhs)\n",
    "            non_terminal_rules.append(tuple([lhs]+rhs))\n",
    "    non_terminal_rules = numpy.array(non_terminal_rules)\n",
    "#     print terminal_rules\n",
    "#     print non_terminal_rules\n",
    "#     print nts\n",
    "    return non_terminal_rules,terminal_rules,nts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binn(rules,nts,lhs,rhs):\n",
    "    if len(rhs)==2:\n",
    "        rules.append(tuple([lhs]+list(rhs)))\n",
    "    else:\n",
    "        global new_var_ct\n",
    "        new_var_ct += 1\n",
    "        temp = \"X\"+str(new_var_ct)\n",
    "        nts.update([temp])\n",
    "        rules.append(tuple([lhs,rhs[0],temp]))\n",
    "        binn(rules,nts,temp,rhs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_binning(non_terminal_rules,nts):\n",
    "    new_rules = []\n",
    "    for rule in non_terminal_rules:\n",
    "        if len(rule)<=3:\n",
    "            new_rules.append(rule)\n",
    "        else:\n",
    "            binn(new_rules,nts,rule[0],rule[1:])\n",
    "    return new_rules,nts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Unit productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_unit_prod(non_terminal_rules,terminal_rules,nts):\n",
    "    while True:\n",
    "        flag = False\n",
    "        new_rules = []\n",
    "        for indx,rule in enumerate(non_terminal_rules):\n",
    "            if len(rule)>2:\n",
    "                new_rules.append(rule)\n",
    "            if len(rule)==2 and rule[1] not in nts: # A->a where a is terminal\n",
    "                new_rules.append(rule)\n",
    "                continue\n",
    "            if len(rule)==2:\n",
    "                flag = True\n",
    "                lhs = rule[0]\n",
    "                rhs = rule[1]\n",
    "                for r in non_terminal_rules:\n",
    "                    if r[0]==rhs:\n",
    "                        new_rule = tuple([lhs]+list(r[1:]))\n",
    "                        new_rules.append(new_rule)\n",
    "                for r in terminal_rules:\n",
    "                    if r[0]==rhs:\n",
    "                        for term in r[1:]:\n",
    "                            new_rule = tuple([lhs,term])\n",
    "                            new_rules.append(new_rule)\n",
    "        non_terminal_rules = new_rules\n",
    "        if flag == False:\n",
    "            break\n",
    "    return non_terminal_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:\n",
      "[('S', 'NP', 'VP') ('S', 'Aux', 'NP', 'VP') ('S', 'VP') ('NP', 'Pronoun')\n",
      " ('NP', 'Proper_Noun') ('NP', 'Det', 'Nominal') ('Nominal', 'Noun')\n",
      " ('Nominal', 'Nominal', 'Noun') ('Nominal', 'Nominal', 'PP') ('VP', 'Verb')\n",
      " ('VP', 'Verb', 'NP') ('VP', 'Verb', 'NP', 'PP') ('VP', 'Verb', 'PP')\n",
      " ('VP', 'VP', 'PP') ('PP', 'Preposition', 'NP')]\n",
      "set(['PP', 'Noun', 'Pronoun', 'Det', 'VP', 'Preposition', 'S', 'Nominal', 'Verb', 'Proper_Noun', 'NP', 'Aux'])\n",
      "AFTER:\n",
      "[('S', 'NP', 'VP'), ('S', 'Aux', 'X1'), ('X1', 'NP', 'VP'), ('S', 'book'), ('S', 'include'), ('S', 'prefer'), ('S', 'Verb', 'NP'), ('S', 'Verb', 'X2'), ('S', 'Verb', 'PP'), ('S', 'VP', 'PP'), ('NP', 'I'), ('NP', 'she'), ('NP', 'me'), ('NP', 'Houston'), ('NP', 'TWA'), ('NP', 'Det', 'Nominal'), ('Nominal', 'book'), ('Nominal', 'flight'), ('Nominal', 'meal'), ('Nominal', 'money'), ('Nominal', 'Nominal', 'Noun'), ('Nominal', 'Nominal', 'PP'), ('VP', 'book'), ('VP', 'include'), ('VP', 'prefer'), ('VP', 'Verb', 'NP'), ('VP', 'Verb', 'X2'), ('X2', 'NP', 'PP'), ('VP', 'Verb', 'PP'), ('VP', 'VP', 'PP'), ('PP', 'Preposition', 'NP')]\n",
      "set(['PP', 'Noun', 'Pronoun', 'Det', 'X2', 'VP', 'Preposition', 'S', 'Nominal', 'Verb', 'Proper_Noun', 'NP', 'Aux', 'X1'])\n"
     ]
    }
   ],
   "source": [
    "new_var_ct = 0\n",
    "non_terminal_rules,terminal_rules,nts = load_grammar()\n",
    "print \"BEFORE:\"\n",
    "print non_terminal_rules\n",
    "print nts\n",
    "non_terminal_rules,nts = do_binning(non_terminal_rules,nts)\n",
    "non_terminal_rules = remove_unit_prod(non_terminal_rules,terminal_rules,nts)\n",
    "print \"AFTER:\"\n",
    "print non_terminal_rules\n",
    "print nts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify Non-terminal and Terminal rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('S', 'NP', 'VP'), ('S', 'Aux', 'X1'), ('X1', 'NP', 'VP'), ('S', 'book'), ('S', 'include'), ('S', 'prefer'), ('S', 'Verb', 'NP'), ('S', 'Verb', 'X2'), ('S', 'Verb', 'PP'), ('S', 'VP', 'PP'), ('NP', 'I'), ('NP', 'she'), ('NP', 'me'), ('NP', 'Houston'), ('NP', 'TWA'), ('NP', 'Det', 'Nominal'), ('Nominal', 'book'), ('Nominal', 'flight'), ('Nominal', 'meal'), ('Nominal', 'money'), ('Nominal', 'Nominal', 'Noun'), ('Nominal', 'Nominal', 'PP'), ('VP', 'book'), ('VP', 'include'), ('VP', 'prefer'), ('VP', 'Verb', 'NP'), ('VP', 'Verb', 'X2'), ('X2', 'NP', 'PP'), ('VP', 'Verb', 'PP'), ('VP', 'VP', 'PP'), ('PP', 'Preposition', 'NP'), ('Det', 'that'), ('Det', 'this'), ('Det', 'a'), ('Noun', 'book'), ('Noun', 'flight'), ('Noun', 'meal'), ('Noun', 'money'), ('Verb', 'book'), ('Verb', 'include'), ('Verb', 'prefer'), ('Pronoun', 'I'), ('Pronoun', 'she'), ('Pronoun', 'me'), ('Proper_Noun', 'Houston'), ('Proper_Noun', 'TWA'), ('Aux', 'does'), ('Preposition', 'from'), ('Preposition', 'to'), ('Preposition', 'on'), ('Preposition', 'near'), ('Preposition', 'through')]\n"
     ]
    }
   ],
   "source": [
    "new_term_rules = []\n",
    "for rule in terminal_rules:\n",
    "    lhs = rule[0]\n",
    "    for rhs in rule[1:]:\n",
    "        new_term_rules.append(tuple([lhs,rhs]))\n",
    "terminal_rules = new_term_rules\n",
    "rules = non_terminal_rules + terminal_rules\n",
    "print rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CYK Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I book a flight from TWA to Houston\n",
      "1) S -> NP VP  (0)\n",
      "2) NP -> I  (1)\n",
      "3) VP -> Verb NP  (1)\n",
      "4) Verb -> book  (3)\n",
      "5) NP -> Det Nominal  (3)\n",
      "6) Det -> a  (5)\n",
      "7) Nominal -> Nominal PP  (5)\n",
      "8) Nominal -> Nominal PP  (7)\n",
      "9) Nominal -> flight  (8)\n",
      "10) PP -> Preposition NP  (7)\n",
      "11) Preposition -> to  (8)\n",
      "12) NP -> Houston  (8)\n",
      "13) PP -> Preposition NP  (8)\n",
      "14) Preposition -> from  (13)\n",
      "15) NP -> TWA  (13)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I book a flight from TWA to Houston\"\n",
    "# Create chart\n",
    "words = sentence.split()\n",
    "def get_lhs(term):\n",
    "    nts = []\n",
    "    for rule in rules:\n",
    "        if len(rule)==2 and rule[1]==term:\n",
    "            nts.append(rule[0])\n",
    "    return nts\n",
    "def get_lhs_nt(ntl,ntr):\n",
    "    nts = []\n",
    "    for rule in rules:\n",
    "        if len(rule)==3 and rule[1]==ntl and rule[2]==ntr:\n",
    "            nts.append(rule[0])\n",
    "    return nts\n",
    "def print_chart():\n",
    "    for i in range(len(words)):\n",
    "        print(\"LEVEL %d\"%(i))\n",
    "        print c[i]\n",
    "        print words_seg[i]\n",
    "def show_parse(parent_prod_seq,parse_q1,parse_q2):\n",
    "#     print \"NEW PARSING LEVEL:\"\n",
    "#     print parent_prod_seq, parse_q1\n",
    "    global bptr\n",
    "    global prod_seq\n",
    "    original_prod_seq = prod_seq + 1\n",
    "    for entry in parse_q1:\n",
    "        lhs = entry[0]\n",
    "        rhs_l = entry[3]\n",
    "        rhs_r = entry[6]\n",
    "        prod_seq += 1\n",
    "        print(\"%d) %s -> %s %s  (%d)\"%(prod_seq,lhs,rhs_l,rhs_r,parent_prod_seq))\n",
    "        for tpl in bptr[entry[1]][entry[2]]:\n",
    "            if tpl[0]==rhs_l:\n",
    "                if entry[1]==0:\n",
    "                    prod_seq +=1\n",
    "                    print(\"%d) %s -> %s  (%d)\"%(prod_seq,tpl[0],tpl[1],original_prod_seq))\n",
    "                else:\n",
    "                    parse_q2.append(tpl)\n",
    "                break\n",
    "#         print bptr[entry[4]][entry[5]]\n",
    "        for tpl in bptr[entry[4]][entry[5]]:\n",
    "            if tpl[0]==rhs_r:\n",
    "                if entry[4]==0:\n",
    "                    prod_seq +=1\n",
    "                    print(\"%d) %s -> %s  (%d)\"%(prod_seq,tpl[0],tpl[1],original_prod_seq))\n",
    "                else:\n",
    "                    parse_q2.append(tpl)\n",
    "                break\n",
    "    parse_q1 = []\n",
    "    if len(parse_q2):\n",
    "        show_parse(original_prod_seq, parse_q2, parse_q1)\n",
    "def create_chart():\n",
    "    global c\n",
    "    global words_seg\n",
    "    c[0] = [[] for i in words]\n",
    "    bptr[0] = [[] for i in words]\n",
    "    words_seg[0] = [[] for i in words]\n",
    "    for i in range(len(words)):\n",
    "        c[0][i] = get_lhs(words[i])\n",
    "        words_seg[0][i] = [words[i]]\n",
    "        for nt in c[0][i]:\n",
    "            bptr[0][i].append(tuple((nt,words[i]))) #[(nt1,\"xyz\"),(nt2,\"uv\"),....]\n",
    "    for i in range(1,len(words)):\n",
    "        c[i] = [[] for l in range(len(words)-i)]\n",
    "        bptr[i] = [[] for l in range(len(words)-i)]\n",
    "        words_seg[i] = [[] for l in range(len(words)-i)]\n",
    "        for j in range(0,len(words)-i):\n",
    "            for k in range(i):\n",
    "                rhs_l = c[k][j]\n",
    "#                 print \"rhs_l \",rhs_l\n",
    "                rhs_r = c[i-k-1][j+k+1]\n",
    "#                 print \"rhs_r\",rhs_r\n",
    "                if len(rhs_l) and len(rhs_r):\n",
    "                    for nt1 in rhs_l:\n",
    "                        for nt2 in rhs_r:\n",
    "#                             print i,j,k,nt1,nt2,get_lhs_nt(nt1,nt2)\n",
    "                            temp = get_lhs_nt(nt1,nt2)\n",
    "                            if temp!=[]:\n",
    "                                for lhs in temp:\n",
    "                                    if lhs not in c[i][j]:\n",
    "                                        # add tuple(lhs,chart_lvl_left,starting_word_no,non_term,chart_lvl_rt,starting_word_no,non_term)\n",
    "                                        bptr[i][j].append(tuple((lhs,k,j,nt1,i-k-1,j+k+1,nt2)))\n",
    "                            c[i][j] += temp\n",
    "            c[i][j]=list(set(c[i][j]))\n",
    "            if len(c[i][j]):\n",
    "                words_seg[i][j] = words[j:j+i+1]\n",
    "c = [[] for i in range(len(words))]\n",
    "words_seg = [[] for i in range(len(words))]\n",
    "bptr = [[] for i in range(len(words))]\n",
    "create_chart()\n",
    "# print_chart()\n",
    "parse_Q1 = []\n",
    "parse_Q2 = []\n",
    "for entry in bptr[len(words)-1][0]:\n",
    "    if entry[0]=='S':\n",
    "        parse_Q1.append(entry)\n",
    "# print bptr\n",
    "prod_seq = 0\n",
    "print \"Example Sentence:\",sentence\n",
    "show_parse(prod_seq,parse_Q1,parse_Q2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
