{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a sentence, create terminal rules from lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lexicon(sentence,pos_given=0):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # Remove punctuation\n",
    "    if re.match('\\W+',tokens[-1])!=None:\n",
    "        tokens = tokens[:len(tokens)-1]\n",
    "        sentence = ' '.join(tokens)\n",
    "    print sentence\n",
    "    if pos_given==0:\n",
    "        token_pos_tuples = pos_tag(tokens)\n",
    "    else:\n",
    "        token_pos_tuples = [nltk.tag.str2tuple(token) for token in tokens]\n",
    "    print token_pos_tuples\n",
    "    outfile = open('lexicon_new.txt','w')\n",
    "    pattern_Det = re.compile('^DT$')\n",
    "    pattern_Noun = re.compile('^NN.*')\n",
    "    pattern_Adjective = re.compile('JJ.*')\n",
    "    pattern_Pronoun = re.compile('PR.*')\n",
    "    pattern_Verb = re.compile('VB.*')\n",
    "    pattern_Preposition = re.compile('IN|TO')\n",
    "    pos_dict = {'Noun':[],'Det':[],'Verb':[],'Pronoun':[],'Proper_Noun':[],'Preposition':[],'Aux':[]}\n",
    "    for tpl in token_pos_tuples:\n",
    "        if pattern_Det.match(tpl[1])!=None:\n",
    "            pos_dict['Det'].append(tpl[0])\n",
    "        elif pattern_Noun.match(tpl[1])!=None:\n",
    "            if re.match('NNP.*',tpl[1])!=None:\n",
    "                pos_dict['Proper_Noun'].append(tpl[0])\n",
    "            else:\n",
    "                pos_dict['Noun'].append(tpl[0])\n",
    "        elif pattern_Adjective.match(tpl[1])!=None: #Treat adjective as noun wlog\n",
    "            pos_dict['Noun'].append(tpl[0])\n",
    "        elif pattern_Pronoun.match(tpl[1])!=None:\n",
    "            pos_dict['Pronoun'].append(tpl[0])\n",
    "        elif pattern_Verb.match(tpl[1])!=None:\n",
    "            if tpl[0].lower() in ['do','does']:\n",
    "                pos_dict['Aux'].append(tpl[0])\n",
    "            else:\n",
    "                pos_dict['Verb'].append(tpl[0])\n",
    "        elif pattern_Preposition.match(tpl[1])!=None:\n",
    "            pos_dict['Preposition'].append(tpl[0])\n",
    "    for key in pos_dict.keys():\n",
    "        if pos_dict[key]!=[]:\n",
    "            entry = key+' -> '+'|'.join(pos_dict[key])+'\\n'\n",
    "    #         print(entry)\n",
    "            outfile.write(entry)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_grammar():\n",
    "    grammar_f = \"/home/debojyoti/nlp/assignment3/parse/grammar1.txt\"\n",
    "    lexicon_f = \"/home/debojyoti/nlp/assignment3/parse/lexicon_new.txt\"\n",
    "    pattern = re.compile(\"^(\\w+)\\s*->(.+)\")\n",
    "    terminal_rules = []\n",
    "    non_terminal_rules = []\n",
    "    nts = set([])\n",
    "    with open(lexicon_f) as lexicon:\n",
    "        for line in lexicon.readlines():\n",
    "            g_obj = pattern.match(line)\n",
    "            lhs = g_obj.group(1)\n",
    "            rhs = g_obj.group(2).strip()\n",
    "            rhs = re.split('\\W+',rhs)\n",
    "            terminal_rules.append(tuple([lhs]+rhs))\n",
    "    terminal_rules = numpy.array(terminal_rules)\n",
    "    with open(grammar_f) as grammar:\n",
    "        for line in grammar.readlines():      \n",
    "            g_obj = pattern.match(line)\n",
    "            lhs = g_obj.group(1)\n",
    "            rhs = g_obj.group(2).strip()\n",
    "            rhs = re.split('\\W+',rhs)\n",
    "            nts.update([lhs]+rhs)\n",
    "            non_terminal_rules.append(tuple([lhs]+rhs))\n",
    "#     print terminal_rules\n",
    "#     print non_terminal_rules\n",
    "#     print nts\n",
    "    return non_terminal_rules,terminal_rules,nts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify Non-terminal and Terminal rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unify_rules():\n",
    "    new_term_rules = []\n",
    "    global terminal_rules\n",
    "    global non_terminal_rules\n",
    "    for rule in terminal_rules:\n",
    "        lhs = rule[0]\n",
    "        for rhs in rule[1:]:\n",
    "            new_term_rules.append(tuple([lhs,rhs]))\n",
    "    terminal_rules = new_term_rules\n",
    "    rules = non_terminal_rules + terminal_rules\n",
    "    print rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book/VB that/DT flight/NN\n",
      "[('Book', 'VB'), ('that', 'DT'), ('flight', 'NN')]\n",
      "Book that flight\n",
      "UNIFIED RULES:\n",
      "[('S', 'NP', 'VP'), ('S', 'Aux', 'NP', 'VP'), ('S', 'VP'), ('NP', 'Pronoun'), ('NP', 'Proper_Noun'), ('NP', 'Det', 'Nominal'), ('NP', 'Nominal'), ('Nominal', 'Noun'), ('Nominal', 'Nominal', 'Noun'), ('Nominal', 'Nominal', 'PP'), ('VP', 'Verb'), ('VP', 'Verb', 'NP'), ('VP', 'Verb', 'NP', 'PP'), ('VP', 'Verb', 'PP'), ('VP', 'VP', 'PP'), ('PP', 'Preposition', 'NP'), ('Noun', 'flight'), ('Det', 'that'), ('Verb', 'Book')]\n"
     ]
    }
   ],
   "source": [
    "# sentence = \"I booked a flight from TWA to Houston\"\n",
    "# sentence = \"Does he know him?\"\n",
    "# sentence = \"Students dumped the trash into a bin.\"\n",
    "# sentence = \"Show the menu on Shatabdi from Kanpur to Kolkata\"\n",
    "# build_lexicon(sentence)\n",
    "sentence = \"Show/VB the/DT menu/NN on/IN Shatabdi/NNP from/IN Kanpur/NNP to/TO Kolkata/NNP\"\n",
    "# sentence = \"Book/VB that/DT flight/NN\"\n",
    "build_lexicon(sentence,1) #for tagged sentence\n",
    "sentence = [re.sub(r'(\\w+)/\\w+',r'\\1',word) for word in sentence.split()] #for tagged sentence\n",
    "sentence = ' '.join(sentence) #for tagged sentence\n",
    "print sentence\n",
    "new_var_ct = 0\n",
    "non_terminal_rules,terminal_rules,nts = load_grammar()\n",
    "# print non_terminal_rules\n",
    "# print terminal_rules\n",
    "print \"UNIFIED RULES:\"\n",
    "unify_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_predict(cur_state):\n",
    "    global non_terminal_rules\n",
    "    global states\n",
    "    global next_instance\n",
    "    global protocols\n",
    "    predicted_rules = []\n",
    "    for indx,instance in enumerate(states[cur_state]):\n",
    "        rule = instance[0]\n",
    "        ptr = instance[1]\n",
    "        start_word_no = instance[2]\n",
    "        if len(rule)-1==ptr:\n",
    "            continue\n",
    "        head = rule[ptr+1]\n",
    "        for seq,r in enumerate(non_terminal_rules):\n",
    "            if r[0]==head and seq not in predicted_rules:\n",
    "                states[cur_state].append(tuple((r,0,cur_state)))\n",
    "                predicted_rules.append(seq)\n",
    "                next_instance[cur_state].append(tuple((cur_state,indx)))\n",
    "                protocols[cur_state].append('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_lhs(term):\n",
    "    for rule in terminal_rules:\n",
    "        if rule[1]==term:\n",
    "            return rule[0]\n",
    "    return None\n",
    "def do_scan(cur_state):\n",
    "    global words\n",
    "    global states\n",
    "    global next_instance\n",
    "    global protocols\n",
    "    if cur_state == len(states)-1:\n",
    "        return\n",
    "    current_symbol = get_lhs(words[cur_state])\n",
    "    current_term_rule = tuple((current_symbol,words[cur_state]))\n",
    "    scanned_symbol = []\n",
    "    for indx,instance in enumerate(states[cur_state]):\n",
    "        rule = instance[0]\n",
    "        ptr = instance[1]\n",
    "        start_word_no = instance[2]\n",
    "        if len(rule)-1==ptr:\n",
    "            continue\n",
    "        if rule[ptr+1] == current_symbol:\n",
    "            if current_symbol not in scanned_symbol:\n",
    "                states[cur_state+1].append(tuple((current_term_rule,1,cur_state)))\n",
    "                protocols[cur_state+1].append('scan')\n",
    "                next_instance[cur_state+1].append((0,0)) #(0,0) marks non-term -> term type production\n",
    "                scanned_symbol.append(current_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_complete(cur_state):\n",
    "    global states\n",
    "    global next_instance\n",
    "    global protocols\n",
    "    completed = []\n",
    "    for indx,instance in enumerate(states[cur_state]):\n",
    "        rule = instance[0]\n",
    "        ptr = instance[1]\n",
    "        start_word_no = instance[2]\n",
    "        if len(rule)-1==ptr and tuple((rule[0],start_word_no)) not in completed:\n",
    "            lhs = rule[0]\n",
    "            for indx1,instance in enumerate(states[start_word_no]):\n",
    "                rule_prev = instance[0]\n",
    "                ptr_prev = instance[1]\n",
    "                start_word_no_prev = instance[2]\n",
    "                if len(rule_prev)-1>ptr_prev and rule_prev[ptr_prev+1]==lhs:\n",
    "                    states[cur_state].append(tuple((rule_prev,ptr_prev+1,start_word_no_prev)))\n",
    "                    if(ptr_prev+1>1 and protocols[start_word_no][indx1]=='complete'): #if rhs has processes length >= 2\n",
    "                        next_instance[cur_state].append([next_instance[start_word_no][indx1],tuple((cur_state,indx))]) #if the production already has a complete symbol, add corresponding prod\n",
    "                    else:\n",
    "                        next_instance[cur_state].append(tuple((cur_state,indx)))\n",
    "                    protocols[cur_state].append('complete')\n",
    "            completed.append(tuple((lhs,start_word_no)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_states():\n",
    "    global states\n",
    "    global protocols\n",
    "    global next_instance\n",
    "    for i in range(len(states)):\n",
    "        print(\"-------------STATE %d-------------\"%(i))\n",
    "        for indx,instance in enumerate(states[i]):\n",
    "            print instance[0],instance[1],instance[2],protocols[i][indx],next_instance[i][indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_instance(inst):\n",
    "    global states\n",
    "    global next_instance\n",
    "    if isinstance(inst,types.ListType):\n",
    "        for each in inst:\n",
    "            write_instance(each)\n",
    "    else:\n",
    "        rule = states[inst[0]][inst[1]][0]\n",
    "        print rule[0],'->',rule[1:]\n",
    "        if next_instance[inst[0]][inst[1]]==(0,0):\n",
    "            return\n",
    "        write_instance(next_instance[inst[0]][inst[1]])\n",
    "def write_productions():\n",
    "    global states\n",
    "    global next_instance\n",
    "    start_idx = -1\n",
    "    for idx,instance in enumerate(states[-1]):\n",
    "        if instance[0][0]=='S' and instance[1]==len(instance[0])-1:\n",
    "            start_idx = idx\n",
    "            break\n",
    "    if start_idx==-1:\n",
    "        print 'No valid parse exists'\n",
    "    else:\n",
    "        rule = states[-1][start_idx][0]\n",
    "        print rule[0],'->',rule[1:]\n",
    "        write_instance(next_instance[-1][start_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=(1,2)\n",
    "isinstance(a,types.ListType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earley Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book that flight\n",
      "-------------STATE 0-------------\n",
      "('START', 'S') 0 0 predict (0, 0)\n",
      "('S', 'NP', 'VP') 0 0 predict (0, 0)\n",
      "('S', 'Aux', 'NP', 'VP') 0 0 predict (0, 0)\n",
      "('S', 'VP') 0 0 predict (0, 0)\n",
      "('NP', 'Pronoun') 0 0 predict (0, 1)\n",
      "('NP', 'Proper_Noun') 0 0 predict (0, 1)\n",
      "('NP', 'Det', 'Nominal') 0 0 predict (0, 1)\n",
      "('NP', 'Nominal') 0 0 predict (0, 1)\n",
      "('VP', 'Verb') 0 0 predict (0, 3)\n",
      "('VP', 'Verb', 'NP') 0 0 predict (0, 3)\n",
      "('VP', 'Verb', 'NP', 'PP') 0 0 predict (0, 3)\n",
      "('VP', 'Verb', 'PP') 0 0 predict (0, 3)\n",
      "('VP', 'VP', 'PP') 0 0 predict (0, 3)\n",
      "('Nominal', 'Noun') 0 0 predict (0, 7)\n",
      "('Nominal', 'Nominal', 'Noun') 0 0 predict (0, 7)\n",
      "('Nominal', 'Nominal', 'PP') 0 0 predict (0, 7)\n",
      "-------------STATE 1-------------\n",
      "('Verb', 'Book') 1 0 scan (0, 0)\n",
      "('VP', 'Verb') 1 0 complete (1, 0)\n",
      "('VP', 'Verb', 'NP') 1 0 complete (1, 0)\n",
      "('VP', 'Verb', 'NP', 'PP') 1 0 complete (1, 0)\n",
      "('VP', 'Verb', 'PP') 1 0 complete (1, 0)\n",
      "('S', 'VP') 1 0 complete (1, 1)\n",
      "('VP', 'VP', 'PP') 1 0 complete (1, 1)\n",
      "('START', 'S') 1 0 complete (1, 5)\n",
      "('NP', 'Pronoun') 0 1 predict (1, 2)\n",
      "('NP', 'Proper_Noun') 0 1 predict (1, 2)\n",
      "('NP', 'Det', 'Nominal') 0 1 predict (1, 2)\n",
      "('NP', 'Nominal') 0 1 predict (1, 2)\n",
      "('PP', 'Preposition', 'NP') 0 1 predict (1, 4)\n",
      "('Nominal', 'Noun') 0 1 predict (1, 11)\n",
      "('Nominal', 'Nominal', 'Noun') 0 1 predict (1, 11)\n",
      "('Nominal', 'Nominal', 'PP') 0 1 predict (1, 11)\n",
      "-------------STATE 2-------------\n",
      "('Det', 'that') 1 1 scan (0, 0)\n",
      "('NP', 'Det', 'Nominal') 1 1 complete (2, 0)\n",
      "('Nominal', 'Noun') 0 2 predict (2, 1)\n",
      "('Nominal', 'Nominal', 'Noun') 0 2 predict (2, 1)\n",
      "('Nominal', 'Nominal', 'PP') 0 2 predict (2, 1)\n",
      "-------------STATE 3-------------\n",
      "('Noun', 'flight') 1 2 scan (0, 0)\n",
      "('Nominal', 'Noun') 1 2 complete (3, 0)\n",
      "('NP', 'Det', 'Nominal') 2 1 complete [(2, 0), (3, 1)]\n",
      "('Nominal', 'Nominal', 'Noun') 1 2 complete (3, 1)\n",
      "('Nominal', 'Nominal', 'PP') 1 2 complete (3, 1)\n",
      "('VP', 'Verb', 'NP') 2 0 complete [(1, 0), (3, 2)]\n",
      "('VP', 'Verb', 'NP', 'PP') 2 0 complete [(1, 0), (3, 2)]\n",
      "('S', 'VP') 1 0 complete (3, 5)\n",
      "('VP', 'VP', 'PP') 1 0 complete (3, 5)\n",
      "('START', 'S') 1 0 complete (3, 7)\n",
      "('PP', 'Preposition', 'NP') 0 3 predict (3, 4)\n",
      "S -> ('VP',)\n",
      "VP -> ('Verb', 'NP')\n",
      "Verb -> ('Book',)\n",
      "NP -> ('Det', 'Nominal')\n",
      "Det -> ('that',)\n",
      "Nominal -> ('Noun',)\n",
      "Noun -> ('flight',)\n"
     ]
    }
   ],
   "source": [
    "new_tuple = tuple(('START','S'))\n",
    "# print non_terminal_rules\n",
    "print sentence\n",
    "words = sentence.split()\n",
    "states = [[] for i in range(len(words)+1)]\n",
    "next_instance = [[] for i in range(len(words)+1)]\n",
    "protocols = [[] for i in range(len(words)+1)]\n",
    "states[0].append(tuple((new_tuple,0,0)))\n",
    "protocols[0].append('predict')\n",
    "next_instance[0].append((0,0))\n",
    "for i in range(len(words)+1):\n",
    "    do_complete(i)\n",
    "    do_predict(i)\n",
    "    do_scan(i)\n",
    "print_states()\n",
    "write_productions()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
