{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lexicon(sentence,pos_given=0):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # Remove punctuation\n",
    "    if re.match('\\W+',tokens[-1])!=None:\n",
    "        tokens = tokens[:len(tokens)-1]\n",
    "        sentence = ' '.join(tokens)\n",
    "    print sentence\n",
    "    if pos_given==0:\n",
    "        token_pos_tuples = pos_tag(tokens)\n",
    "    else:\n",
    "        token_pos_tuples = [nltk.tag.str2tuple(token) for token in tokens]\n",
    "    print token_pos_tuples\n",
    "    outfile = open('lexicon_new.txt','w')\n",
    "    pattern_Det = re.compile('^DT$')\n",
    "    pattern_Noun = re.compile('^NN.*')\n",
    "    pattern_Adjective = re.compile('JJ.*')\n",
    "    pattern_Pronoun = re.compile('PR.*')\n",
    "    pattern_Verb = re.compile('VB.*')\n",
    "    pattern_Preposition = re.compile('IN|TO')\n",
    "    pos_dict = {'Noun':[],'Det':[],'Verb':[],'Pronoun':[],'Proper_Noun':[],'Preposition':[],'Aux':[]}\n",
    "    for tpl in token_pos_tuples:\n",
    "        if pattern_Det.match(tpl[1])!=None:\n",
    "            pos_dict['Det'].append(tpl[0])\n",
    "        elif pattern_Noun.match(tpl[1])!=None:\n",
    "            if re.match('NNP.*',tpl[1])!=None:\n",
    "                pos_dict['Proper_Noun'].append(tpl[0])\n",
    "            else:\n",
    "                pos_dict['Noun'].append(tpl[0])\n",
    "        elif pattern_Adjective.match(tpl[1])!=None: #Treat adjective as noun wlog\n",
    "            pos_dict['Noun'].append(tpl[0])\n",
    "        elif pattern_Pronoun.match(tpl[1])!=None:\n",
    "            pos_dict['Pronoun'].append(tpl[0])\n",
    "        elif pattern_Verb.match(tpl[1])!=None:\n",
    "            if tpl[0].lower() in ['do','does']:\n",
    "                pos_dict['Aux'].append(tpl[0])\n",
    "            else:\n",
    "                pos_dict['Verb'].append(tpl[0])\n",
    "        elif pattern_Preposition.match(tpl[1])!=None:\n",
    "            pos_dict['Preposition'].append(tpl[0])\n",
    "    for key in pos_dict.keys():\n",
    "        if pos_dict[key]!=[]:\n",
    "            entry = key+' -> '+'|'.join(pos_dict[key])+'\\n'\n",
    "    #         print(entry)\n",
    "            outfile.write(entry)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_grammar():\n",
    "    grammar_f = \"/home/debojyoti/nlp/assignment3/parse/grammar1.txt\"\n",
    "    lexicon_f = \"/home/debojyoti/nlp/assignment3/parse/lexicon_new.txt\"\n",
    "    pattern = re.compile(\"^(\\w+)\\s*->(.+)\")\n",
    "    terminal_rules = []\n",
    "    non_terminal_rules = []\n",
    "    nts = set([])\n",
    "    with open(lexicon_f) as lexicon:\n",
    "        for line in lexicon.readlines():\n",
    "            g_obj = pattern.match(line)\n",
    "            lhs = g_obj.group(1)\n",
    "            rhs = g_obj.group(2).strip()\n",
    "            rhs = re.split('\\W+',rhs)\n",
    "            terminal_rules.append(tuple([lhs]+rhs))\n",
    "    terminal_rules = numpy.array(terminal_rules)\n",
    "    with open(grammar_f) as grammar:\n",
    "        for line in grammar.readlines():      \n",
    "            g_obj = pattern.match(line)\n",
    "            lhs = g_obj.group(1)\n",
    "            rhs = g_obj.group(2).strip()\n",
    "            rhs = re.split('\\W+',rhs)\n",
    "            nts.update([lhs]+rhs)\n",
    "            non_terminal_rules.append(tuple([lhs]+rhs))\n",
    "#     print terminal_rules\n",
    "#     print non_terminal_rules\n",
    "#     print nts\n",
    "    return non_terminal_rules,terminal_rules,nts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unify_rules():\n",
    "    new_term_rules = []\n",
    "    global terminal_rules\n",
    "    global non_terminal_rules\n",
    "    for rule in terminal_rules:\n",
    "        lhs = rule[0]\n",
    "        for rhs in rule[1:]:\n",
    "            new_term_rules.append(tuple([lhs,rhs]))\n",
    "    terminal_rules = new_term_rules\n",
    "    rules = non_terminal_rules + terminal_rules\n",
    "    print rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show/VB the/DT menu/NN on/IN Shatabdi/NNP from/IN Kanpur/NNP to/TO Kolkata/NNP\n",
      "[('Show', 'VB'), ('the', 'DT'), ('menu', 'NN'), ('on', 'IN'), ('Shatabdi', 'NNP'), ('from', 'IN'), ('Kanpur', 'NNP'), ('to', 'TO'), ('Kolkata', 'NNP')]\n",
      "Show the menu on Shatabdi from Kanpur to Kolkata\n",
      "UNIFIED RULES:\n",
      "[('Noun', 'menu'), ('Det', 'the'), ('Preposition', 'on'), ('Preposition', 'from'), ('Preposition', 'to'), ('Verb', 'Show'), ('Proper_Noun', 'Shatabdi'), ('Proper_Noun', 'Kanpur'), ('Proper_Noun', 'Kolkata')]\n",
      "[('S', 'NP', 'VP'), ('S', 'Aux', 'NP', 'VP'), ('S', 'VP'), ('NP', 'Pronoun'), ('NP', 'Proper_Noun'), ('NP', 'Det', 'Nominal'), ('NP', 'Nominal'), ('Nominal', 'Noun'), ('Nominal', 'Nominal', 'Noun'), ('Nominal', 'Nominal', 'PP'), ('VP', 'Verb'), ('VP', 'Verb', 'NP'), ('VP', 'Verb', 'NP', 'PP'), ('VP', 'Verb', 'PP'), ('VP', 'VP', 'PP'), ('PP', 'Preposition', 'NP')]\n",
      "[('S', 'NP', 'VP'), ('S', 'Aux', 'NP', 'VP'), ('S', 'VP'), ('NP', 'Pronoun'), ('NP', 'Proper_Noun'), ('NP', 'Det', 'Nominal'), ('NP', 'Nominal'), ('Nominal', 'Noun'), ('Nominal', 'Nominal', 'Noun'), ('Nominal', 'Nominal', 'PP'), ('VP', 'Verb'), ('VP', 'Verb', 'NP'), ('VP', 'Verb', 'NP', 'PP'), ('VP', 'Verb', 'PP'), ('VP', 'VP', 'PP'), ('PP', 'Preposition', 'NP'), ('Noun', 'menu'), ('Det', 'the'), ('Preposition', 'on'), ('Preposition', 'from'), ('Preposition', 'to'), ('Verb', 'Show'), ('Proper_Noun', 'Shatabdi'), ('Proper_Noun', 'Kanpur'), ('Proper_Noun', 'Kolkata')]\n"
     ]
    }
   ],
   "source": [
    "# sentence = \"I booked a flight from TWA to Houston\"\n",
    "# sentence = \"Does he know him?\"\n",
    "# sentence = \"Students dumped the trash into a bin.\"\n",
    "# sentence = \"Show the menu on Shatabdi from Kanpur to Kolkata\"\n",
    "# build_lexicon(sentence)\n",
    "sentence = \"Show/VB the/DT menu/NN on/IN Shatabdi/NNP from/IN Kanpur/NNP to/TO Kolkata/NNP\"\n",
    "build_lexicon(sentence,1) #for tagged sentence\n",
    "sentence = [re.sub(r'(\\w+)/\\w+',r'\\1',word) for word in sentence.split()] #for tagged sentence\n",
    "sentence = ' '.join(sentence) #for tagged sentence\n",
    "print sentence\n",
    "new_var_ct = 0\n",
    "non_terminal_rules,terminal_rules,nts = load_grammar()\n",
    "# print non_terminal_rules\n",
    "# print terminal_rules\n",
    "print \"UNIFIED RULES:\"\n",
    "unify_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
